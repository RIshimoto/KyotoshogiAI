# -*- coding: utf-8 -*-
"""pv_mcts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bAcM6668cnMefQOrkttr9BbbA4hC6NZA
"""
from game import State
from dual_network import DN_INPUT_SHAPE
from math import sqrt
from tensorflow.keras.models import load_model
from pathlib import Path
import numpy as np

# パラメータの準備
PV_EVALUATE_COUNT = 50 #1推論あたりのシミュレーション回数

def predict(model, state):
  # 推論のための入力データのシェイプの変換
  a, b, c = DN_INPUT_SHAPE
  x = np.array(state.pieces_array())
  x = x.reshape(c, a, b).transpose(1, 2, 0).reshape(1, a, b, c)

  # 推論
  y = model.predict(x, batch_size=1)

  # 方策の取得
  policies = y[0][0][list(state.legal_actions())]
  policies /= sum(policies) if sum(policies) else 1

  # 価値の取得
  value = y[1][0][0]
  return policies, value

def nodes_to_scores(nodes):
  scores = []
  for c in nodes:
    scores.append(c.n)
  return scores

def pv_mcts_scores(model, state, temperature):
  class Node:
    def __init__(self, state, p):
      self.state = state #状態
      self.p = p # 方策
      self.w = 0 # 累計価値
      self.n = 0 # 試行回数
      self.child_nodes = None # ノード群
    
    def evaluate(self):
      if self.state.is_done():
        value = -1 if self.state.is_lose() else 0

        self.w += value
        self.n += 1
        return value
      
      #子ノードが存在しないとき
      if not self.child_nodes:
        #　ニューラルネットワークの推論で方策と価値の取得
        policies, value = predict(model, self.state)
        # 累計価値と試行回数の更新
        self.w += value
        self.n += 1

        # 子ノードの展開
        self.child_nodes = []
        for action, policy in zip(self.state.legal_actions(), policies):
          self.child_nodes.append(Node(self.state.next(action), policy))
        return value
      # 子ノードが存在するとき
      else:
        value = -self.next_child_node().evaluate()

        self.w += value
        self.n += 1
        return value
    
    def next_child_node(self):
      C_PUCT = 1.0
      t = sum(nodes_to_scores(self.child_nodes))
      pucb_values = []
      for child_node in self.child_nodes:
        pucb_values.append((-child_node.w / child_node.n if child_node.n else 0.0) + C_PUCT * child_node.p * sqrt(t) / (1 + child_node.n))
      return self.child_nodes[np.argmax(pucb_values)]

  # 現在の局面のノードの作成
  root_node = Node(state, 0)

  for _ in range(PV_EVALUATE_COUNT):
    root_node.evaluate()
  
  # 合法手の確率分布
  scores = nodes_to_scores(root_node.child_nodes)
  if temperature == 0:# 最大値のみ1
    action = np.argmax(scores)
    scores = np.zeros(len(scores))
    scores[action] = 1
  else: # ボルツマン分布でバラつき付加
    scores = boltzman(scores, temperature)
  return scores

def pv_mcts_action(model, temperature=0):
  def pv_mcts_action(state):
    scores = pv_mcts_scores(model, state, temperature)
    return np.random.choice(state.legal_actions(), p=scores)
  return pv_mcts_action

def boltzman(xs, temperature):
  xs = [x ** (1 / temperature) for x in xs]
  return [x / sum(xs) for x in xs]

if __name__ == '__main__':
  path = sorted(Path('./model').glob('*.h5'))[-1]
  model = load_model(str(path))

  state = State()
  
  next_action = pv_mcts_action(model, 1.0)

  while True:
    if state.is_done():
      break
    
    action = next_action(state)

    state = state.next(action)
    #print(state)
